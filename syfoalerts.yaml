apiVersion: "nais.io/v1"
kind: "Alert"
metadata:
  name: syfoalerts
  namespace: default
  labels:
    team: teamsykefravr
spec:
  receivers:
    slack:
      channel: '#syfo-alerts-prod'
      #prependText: '<!here> | '
  alerts:
    - alert: syfo-app-nede
      expr: kube_deployment_status_replicas_available{deployment=~"syfoarbeidsgivertilgang|syfoservicestrangler|syfosyketilfelle|syfotekster",job="kubernetes-pods"} == 0
      for: 2m
      description: "{{ $labels.deployment }} er nede"
      action: "Se `kubectl kubectl get pod  | grep {{ $labels.deployment }}` for å se podder, og `kubectl logs $podnavn` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfo-asynkron-app-nede
      expr: kube_deployment_status_replicas_available{deployment=~"pdf-gen"} == 0
      for: 10m
      description: "{{ $labels.deployment }} er nede"
      action: "Se `kubectl kubectl get pod  | grep {{ $labels.deployment }}` for å se podder, og `kubectl logs $podnavn` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfo-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"syfoarbeidsgivertilgang|syfoservicestrangler|syfosyketilfelle|pdf-gen"}[30m])) by (container) > 2
      for: 5m
      description: "{{ $labels.container }} har restartet flere ganger siste halvtimen!"
      action: "Se `kubectl describe pod {{ $labels.container }}` for events, og `kubectl logs {{ $labels.container }}` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfo-mangler-metrikker
      expr: absent(up{app=~"pdf-gen|syfoarbeidsgivertilgang|syfoservicestrangler|syfosyketilfelle|syfotekster",job="kubernetes-pods"})
      for: 5m
      description: "{{ $labels.app }} rapporterer ingen metrikker i {{ $labels.kubernetes_namespace }}"
      action: "Sjekk om {{ $labels.app }} i {{ $labels.kubernetes_namespace }} er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfosyketilfelle-lytter-stoppet
      expr: sum(increase(syfosyketilfelle_kafkalytter_stoppet_total[30m])) > 0
      for: 5m
      description: "{{ $labels.app }} sin kafkalytter har stoppet i {{ $labels.kubernetes_namespace }}"
      action: "Kafkalytter stoppet på syfosyketilfelle. Sjekk logger, og evt. `kubectl delete pod {{ $labels.kubernetes_pod_name }}` for å restarte"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfoservicestrangler-lytter-stoppet
      expr: sum(increase(syfoservicestrangler_kafkalytter_stoppet_total[30m])) > 0
      for: 5m
      description: "{{ $labels.app }} sin kafkalytter har stoppet i {{ $labels.kubernetes_namespace }}"
      action: "Kafkalytter stoppet på syfoservicestrangler. Sjekk logger, og evt. `kubectl delete pod {{ $labels.kubernetes_pod_name }}` for å restarte"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: syfo-errorlogging
      expr: (100 * sum by (log_app) (rate(logd_messages_total{log_app=~"pdf-gen|syfoarbeidsgivertilgang|syfoservicestrangler|syfosyketilfelle",log_level="Error"}[5m])) / sum by (log_app) (rate(logd_messages_total{log_app=~"pdf-gen|syfoarbeidsgivertilgang|syfoservicestrangler|syfosyketilfelle"}[5m]))) > 10
      for: 3m
      description: "{{ $labels.log_app }} rapporterer error i loggene"
      action: "Sjekk loggene til {{ $labels.log_app }} i {{ $labels.log_namespace }}, for å se hvorfor det er så mye feil"
